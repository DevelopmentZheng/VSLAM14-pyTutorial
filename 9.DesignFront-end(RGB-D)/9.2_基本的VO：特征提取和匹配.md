&emsp;
# 9.2 基本的VO：特征提取和匹配

下面我们来实现 $VO$，先来考虑特征点法。它 $VO$ 任务是，根据输入的图像，计算相机运动和特征点位置。前面我们都讨论的是在两两帧间的位姿估计，然而我们将发现仅凭两帧的估计是不够的。我们会把特征点缓存成一个小地图，计算当前帧与地图之间的位置关系。但那样程序会复杂一些，所以，让我们先订个小目标，暂时从两两帧间的运动估计出发。

&emsp;
## 9.2.1 两两帧的视觉里程计

如果像前面两章一样，只关心两个帧之间的运动估计，并且不优化特征点的位置。然而把估得的位姿“串”起来，也能得到一条运动轨迹。这种方式可以看成两两帧间的（Pairwise），`无结构（Structureless）`的 VO，实现起来最为简单，但是效果不佳。为什么不佳呢？我们带着读者来体验一下。记该工程为 0.2 版本。


<div align="center">
    <image src="./imgs/9.2-1.png" width = 600>
</div>
&emsp;


两两帧之间的 $VO$ 工作示意图如图 9-4 所示。在这种 $VO$ 里，我们定义了 `参考帧（Reference）` 和 `当前帧（Current）` 这两个概念。以参考帧为坐标系，我们把当前帧与它进行特征匹配，并估计运动关系。假设
- 参考帧相对世界坐标的变换矩阵为 $T_{rw}$
- 当前帧与世界坐标系间为 $T_{cw}$

则待估计的运动与这两个帧的变换矩阵构成左乘关系：

$$\pmb{T}_{cr}，s.t.，\pmb{T}_{cw} = \pmb{T}_{cr}\pmb{T}_{rw}$$


在 $t - 1$ 到 $t$ 时刻，我们以 $t - 1$ 为参考，求取 $t$ 时刻的运动。这可以通过特征点匹配、光流或直接法得到，但这里我们只关心运动，不关心结构。换句话说，只要通过特征点成功求出了运动，我们就不再需要这帧的特征点了。这种做法当然会有缺陷，但是忽略掉数量庞大的特征点可以节省许多的计算量。然后，在 $t$ 到 $t + 1 时刻$，我们又以 $t$ 时刻为参考帧，考虑 $t$ 到 $t + 1$ 间的运动关系。如此往复，就得到了一条运动轨迹。

这种 $VO$ 的工作方式是简单的，不过实现也可以有若干种。我们以传统的匹配特征点——求 $PnP$ 的方法为例实现一遍。希望读者能够结合之前章的知识，自己实现一下光流/直接法或 ICP 求运动的 VO。在匹配特征点的方式中，最重要的参考帧与当前帧之间的特征匹配关系，它的流程可归纳如下：


1. 对新来的当前帧，提取关键点和描述子。
2. 如果系统未初始化，以该帧为参考帧，根据深度图计算关键点的 3D 位置，返回1。
3. 估计参考帧与当前帧间的运动。
4. 判断上述估计是否成功。
5. 若成功，把当前帧作为新的参考帧，回 1。
6. 若失败，计连续丢失帧数。当连续丢失超过一定帧数，置 VO 状态为丢失，算法结束。若未超过，返回 1


VisualOdometry 类给出了上述算法的实现。

>visual_odometry.h
```c++
#ifndef VISUAL_ODOMETRY_H
#define VISUAL_ODOMETRY_H

#include "myslam/common_include.h"
#include "myslam/map.h"

#include <opencv2/features2d/features2d.hpp>

namespace myslam
{
    class VisualOdometry
    {
        public:
            typedef shared_ptr<VisualOdometry> Ptr;
            enum VOState{
                INITIALIZING = -1,
                OK = 0,
                LOST
            };

            VOState     state_; // current VO status
            Map::Ptr    map_;   // map with all frames and map points
            Frame::Ptr  ref_;   // reference frame 
            Frame::Ptr  curr_;  // current frame 

            cv::Ptr<cv::ORB> orb_;  // orb detector and computer 
            vector<cv::Point3f>     pts_3d_ref_;        // 3d points in reference frame 
            vector<cv::KeyPoint>    keypoints_curr_;    // keypoints in current frame
            cv::Mat                 descriptors_curr_;  // descriptor in current frame 
            cv::Mat                 descriptors_ref_;   // descriptor in reference frame 
            vector<cv::DMatch>      feature_matches_;

            SE3 T_c_r_estimated_;   // the estimated pose of current frame 
            int num_inliers_;       // number of inlier features in icp
            int num_lost_;          // number of lost times

            // parameters
            int num_of_features_;   // number of features
            double scale_factor_;   // scale in image pyramid
            int level_pyramid_;     // number of pyramid levels
            float match_ratio_;     // ratio for selecting  good matches
            int max_num_lost_;      // max number of continuous lost times
            int min_inliers_;       // minimum inliers

            double key_frame_min_rot;   // minimal rotation of two key-frames
            double key_frame_min_trans; // minimal translation of two key-frames

        public:
            VisualOdometry();
            ~VisualOdometry();

            bool addFrame(Frame::Ptr frame);    // add a new frame 
        
        protected:
            // inner operation
            void extractKeyPoints();
            void computeDescriptors();
            void featureMatching();
            void poseEstimationPnP();
            void setRef3DPoints();

            void addKeyFrame();
            bool checkEstimatedPose();
            bool checkKeyFrame();
    };
}

#endif // VISUAL_ODOMETRY_H
```

>visual_odometry.cpp
```c++

#include <opencv2/highgui/highgui.hpp>
#include <opencv2/imgproc/imgproc.hpp>
#include <opencv2/calib3d/calib3d.hpp>
#include <algorithm>
#include <boost/timer.hpp>

#include "myslam/config.h"
#include "myslam/visual_odometry.h"

namespace myslam
{

VisualOdometry::VisualOdometry() :
    state_ ( INITIALIZING ), ref_ ( nullptr ), curr_ ( nullptr ), map_ ( new Map ), num_lost_ ( 0 ), num_inliers_ ( 0 )
{
    num_of_features_    = Config::get<int> ( "number_of_features" );
    scale_factor_       = Config::get<double> ( "scale_factor" );
    level_pyramid_      = Config::get<int> ( "level_pyramid" );
    match_ratio_        = Config::get<float> ( "match_ratio" );
    max_num_lost_       = Config::get<float> ( "max_num_lost" );
    min_inliers_        = Config::get<int> ( "min_inliers" );
    key_frame_min_rot   = Config::get<double> ( "keyframe_rotation" );
    key_frame_min_trans = Config::get<double> ( "keyframe_translation" );
    orb_ = cv::ORB::create ( num_of_features_, scale_factor_, level_pyramid_ );
}

VisualOdometry::~VisualOdometry()
{

}

bool VisualOdometry::addFrame ( Frame::Ptr frame )
{
    switch ( state_ )
    {
    case INITIALIZING:
    {
        state_ = OK;
        curr_ = ref_ = frame;
        map_->insertKeyFrame ( frame );
        // extract features from first frame 
        extractKeyPoints();
        computeDescriptors();
        // compute the 3d position of features in ref frame 
        setRef3DPoints();
        break;
    }
    case OK:
    {
        curr_ = frame;
        extractKeyPoints();
        computeDescriptors();
        featureMatching();
        poseEstimationPnP();
        if ( checkEstimatedPose() == true ) // a good estimation
        {
            curr_->T_c_w_ = T_c_r_estimated_ * ref_->T_c_w_;  // T_c_w = T_c_r*T_r_w 
            ref_ = curr_;
            setRef3DPoints();
            num_lost_ = 0;
            if ( checkKeyFrame() == true ) // is a key-frame
            {
                addKeyFrame();
            }
        }
        else // bad estimation due to various reasons
        {
            num_lost_++;
            if ( num_lost_ > max_num_lost_ )
            {
                state_ = LOST;
            }
            return false;
        }
        break;
    }
    case LOST:
    {
        cout<<"vo has lost."<<endl;
        break;
    }
    }

    return true;
}

void VisualOdometry::extractKeyPoints()
{
    orb_->detect ( curr_->color_, keypoints_curr_ );
}

void VisualOdometry::computeDescriptors()
{
    orb_->compute ( curr_->color_, keypoints_curr_, descriptors_curr_ );
}

void VisualOdometry::featureMatching()
{
    // match desp_ref and desp_curr, use OpenCV's brute force match 
    vector<cv::DMatch> matches;
    cv::BFMatcher matcher ( cv::NORM_HAMMING );
    matcher.match ( descriptors_ref_, descriptors_curr_, matches );
    // select the best matches
    float min_dis = std::min_element (
                        matches.begin(), matches.end(),
                        [] ( const cv::DMatch& m1, const cv::DMatch& m2 )
    {
        return m1.distance < m2.distance;
    } )->distance;

    feature_matches_.clear();
    for ( cv::DMatch& m : matches )
    {
        if ( m.distance < max<float> ( min_dis*match_ratio_, 30.0 ) )
        {
            feature_matches_.push_back(m);
        }
    }
    cout<<"good matches: "<<feature_matches_.size()<<endl;
}

void VisualOdometry::setRef3DPoints()
{
    // select the features with depth measurements 
    pts_3d_ref_.clear();
    descriptors_ref_ = Mat();
    for ( size_t i=0; i<keypoints_curr_.size(); i++ )
    {
        double d = ref_->findDepth(keypoints_curr_[i]);               
        if ( d > 0)
        {
            Vector3d p_cam = ref_->camera_->pixel2camera(
                Vector2d(keypoints_curr_[i].pt.x, keypoints_curr_[i].pt.y), d
            );
            pts_3d_ref_.push_back( cv::Point3f( p_cam(0,0), p_cam(1,0), p_cam(2,0) ));
            descriptors_ref_.push_back(descriptors_curr_.row(i));
        }
    }
}

void VisualOdometry::poseEstimationPnP()
{
    // construct the 3d 2d observations
    vector<cv::Point3f> pts3d;
    vector<cv::Point2f> pts2d;
    
    for ( cv::DMatch m:feature_matches_ )
    {
        pts3d.push_back( pts_3d_ref_[m.queryIdx] );
        pts2d.push_back( keypoints_curr_[m.trainIdx].pt );
    }
    
    Mat K = ( cv::Mat_<double>(3,3)<<
        ref_->camera_->fx_, 0, ref_->camera_->cx_,
        0, ref_->camera_->fy_, ref_->camera_->cy_,
        0,0,1
    );
    Mat rvec, tvec, inliers;
    cv::solvePnPRansac( pts3d, pts2d, K, Mat(), rvec, tvec, false, 100, 4.0, 0.99, inliers );
    num_inliers_ = inliers.rows;
    cout<<"pnp inliers: "<<num_inliers_<<endl;
    T_c_r_estimated_ = SE3(
        SO3(rvec.at<double>(0,0), rvec.at<double>(1,0), rvec.at<double>(2,0)), 
        Vector3d( tvec.at<double>(0,0), tvec.at<double>(1,0), tvec.at<double>(2,0))
    );
}

bool VisualOdometry::checkEstimatedPose()
{
    // check if the estimated pose is good
    if ( num_inliers_ < min_inliers_ )
    {
        cout<<"reject because inlier is too small: "<<num_inliers_<<endl;
        return false;
    }
    // if the motion is too large, it is probably wrong
    Sophus::Vector6d d = T_c_r_estimated_.log();
    if ( d.norm() > 5.0 )
    {
        cout<<"reject because motion is too large: "<<d.norm()<<endl;
        return false;
    }
    return true;
}

bool VisualOdometry::checkKeyFrame()
{
    Sophus::Vector6d d = T_c_r_estimated_.log();
    Vector3d trans = d.head<3>();
    Vector3d rot = d.tail<3>();
    if ( rot.norm() >key_frame_min_rot || trans.norm() >key_frame_min_trans )
        return true;
    return false;
}

void VisualOdometry::addKeyFrame()
{
    cout<<"adding a key-frame"<<endl;
    map_->insertKeyFrame ( curr_ );
}

}
```

关于这个 VisualOdometry 类，有几点需要解释：
1. VO 本身有若干种状态：设定第一帧、顺利跟踪或丢失，你可以把它看成一个 `有限状态机（Finite State Machine, FSM）`。当然状态也可以有更多种，例如单目 VO 至少还有一个初始化状态。在我们的实现中，考虑最简单的三个状态：初始化、正常、丢失。
2. 我们把一些中间变量定义在类中，这样可省去复杂的参数传递。因为它们都是定义在类内部的，所以各个函数都可以访问它们。
3. 特征提取和匹配当中的参数，从参数文件中读取。例如：




&emsp;
## 主函数
>main.cpp
```c++
// -------------- test the visual odometry -------------
#include <fstream>
#include <boost/timer.hpp>
#include <opencv2/imgcodecs.hpp>
#include <opencv2/highgui/highgui.hpp>
#include <opencv2/viz.hpp> 

#include "myslam/config.h"
#include "myslam/visual_odometry.h"

int main ( int argc, char** argv )
{
    myslam::Config::setParameterFile("/home/liheqian/datav/SLAM/test/chapter9/0.2/config/default.yaml");
    myslam::VisualOdometry::Ptr vo ( new myslam::VisualOdometry );

    string dataset_dir = myslam::Config::get<string> ( "dataset_dir" );
    cout<<"dataset: "<<dataset_dir<<endl;
    ifstream fin ( dataset_dir+"/associate.txt" );
    if ( !fin )
    {
        cout<<"please generate the associate file called associate.txt!"<<endl;
        return 1;
    }

    vector<string> rgb_files, depth_files;
    vector<double> rgb_times, depth_times;
    while ( !fin.eof() )
    {
        string rgb_time, rgb_file, depth_time, depth_file;
        fin>>rgb_time>>rgb_file>>depth_time>>depth_file;
        rgb_times.push_back ( atof ( rgb_time.c_str() ) );
        depth_times.push_back ( atof ( depth_time.c_str() ) );
        rgb_files.push_back ( dataset_dir+"/"+rgb_file );
        depth_files.push_back ( dataset_dir+"/"+depth_file );

        if ( fin.good() == false )
            break;
    }

    myslam::Camera::Ptr camera ( new myslam::Camera );
    
    // visualization
    cv::viz::Viz3d vis("Visual Odometry");
    cv::viz::WCoordinateSystem world_coor(1.0), camera_coor(0.5);
    cv::Point3d cam_pos( 0, -1.0, -1.0 ), cam_focal_point(0,0,0), cam_y_dir(0,1,0);
    cv::Affine3d cam_pose = cv::viz::makeCameraPose( cam_pos, cam_focal_point, cam_y_dir );
    vis.setViewerPose( cam_pose );
    
    world_coor.setRenderingProperty(cv::viz::LINE_WIDTH, 2.0);
    camera_coor.setRenderingProperty(cv::viz::LINE_WIDTH, 1.0);
    vis.showWidget( "World", world_coor );
    vis.showWidget( "Camera", camera_coor );

    cout<<"read total "<<rgb_files.size() <<" entries"<<endl;
    for ( int i=0; i<rgb_files.size(); i++ )
    {
        Mat color = cv::imread ( rgb_files[i] );
        Mat depth = cv::imread ( depth_files[i], -1 );
        if ( color.data==nullptr || depth.data==nullptr )
            break;
        myslam::Frame::Ptr pFrame = myslam::Frame::createFrame();
        pFrame->camera_ = camera;
        pFrame->color_ = color;
        pFrame->depth_ = depth;
        pFrame->time_stamp_ = rgb_times[i];

        boost::timer timer;
        vo->addFrame ( pFrame );
        cout<<"VO costs time: "<<timer.elapsed()<<endl;
        
        if ( vo->state_ == myslam::VisualOdometry::LOST )
            break;
        SE3 Tcw = pFrame->T_c_w_.inverse();
        
        // show the map and the camera pose 
        cv::Affine3d M(
            cv::Affine3d::Mat3( 
                Tcw.rotation_matrix()(0,0), Tcw.rotation_matrix()(0,1), Tcw.rotation_matrix()(0,2),
                Tcw.rotation_matrix()(1,0), Tcw.rotation_matrix()(1,1), Tcw.rotation_matrix()(1,2),
                Tcw.rotation_matrix()(2,0), Tcw.rotation_matrix()(2,1), Tcw.rotation_matrix()(2,2)
            ), 
            cv::Affine3d::Vec3(
                Tcw.translation()(0,0), Tcw.translation()(1,0), Tcw.translation()(2,0)
            )
        );
        
        cv::imshow("image", color );
        cv::waitKey(1);
        vis.setWidgetPose( "Camera", M);
        vis.spinOnce(1, false);
    }

    return 0;
}
```

4. addFrame 函数是外部调用的接口。使用 VO 时，将图像数据装入 Frame 类后，调用 addFrame 估计其位姿。该函数根据 VO 所处的状态实现不同的操作：


值得一提的是，由于各种原因，我们设计的上述 VO 算法，每一步都有可能失败。例如图片中不易提特征、特征点缺少深度值、误匹配、运动估计出错等等。因此，要设计一个鲁棒的 VO，必须（最好是显式地）考虑到上述所有可能出错的地方——那自然会使程序变得非常复杂。我们在 checkEstimatedPose 中，根据 `内点（inlier）`的数量以及运动的大小做一个简单的检测：认为内点不可太少，而运动不可能过大。当然，读者也可以思考其他检测问题的手段，尝试一下效果。

我们略去 VisualOdometry 类其余的实现，读者可在 github 上找到所有的源代码。最后，我们在 test 中加入该 VO 的测试程序，使用数据集观察估计的运动效果：

为了运行这个程序，你需要做几件事：
1. 因为我们用 OpenCV3 的 viz 模块显示估计位姿，请确保你安装的是 OpenCV3，并且 viz 模块也编译安装了。
2. 准备 tum 数据集中的其中一个。简单起见，我推荐 fr1_xyz 那一个。请使用 associate.py 生成一个配对文件 associate.txt。关于 tum 数据集格式我们已经在8.3节中介绍过了。
3. 在 config/default.yaml 中填写你的数据集所在路径，参照我的写法即可。然后，用
```
bin/run_vo config/default.yaml
```

执行程序，就可以看到实时的演示了，如图 9-5 所示。

在演示程序中，你可以看到当前帧的图像与它的估计位置。我们画出了世界坐标系的坐标轴（大）与当前帧的坐标轴（小），颜色与轴的对应关系为：蓝色-Z，红色-X，绿色-Y。你可以直观地感受到相机的运动，它大致与我们人类的感觉是相符的，尽管效果离预想还有一定的差距。我还输出了 VO 单次计算的用时，在我的机器上，大约能够以 30 多毫秒左右的速度运行。减少特征点数量可以提高运算速度。读者可以修改运行参数和数据集，看看它在各种情况下的表现。