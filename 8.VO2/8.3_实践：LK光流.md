&emsp;
# 8.3 实践：LK光流
## 8.3.1 使用 TUM 数据集
下面，我们来演示如何用 $OpenCV$ 提供的光流法来跟踪特征点。

>TUM数据集
- 与上一节一样，我们准备了若干张数据集图像，存放在程序目录中的 data/文件夹下。它们来自于慕尼黑工业大学（TUM）提供的公开 $RGB-D$ 数据集。以后我们就称之为 TUM 数据集。
    - 它含有许多个 $RGB-D$ 视频，可以作为 $RGB-D$ 或单目 SLAM 的实验数据
    - 它还提供了用运动捕捉系统测量的精确轨迹，可以作为标准轨迹以校准 SLAM 系统
- TUM 官网：https://vision.in.tum.de/data/datasets/rgbd-dataset

由于该数据集比较大，我们没有放到 github 上（否则下载代码的读者要等待很长时间），请读者去数据集主页找到对应的数据。本程序中使用了一部分“freburg1_desk”数据集中的图像。读者可以在 TUM 数据集主页找到它的下载链接。或者，也可以直接使用本书在 github 上提供的部分。

我们的数据位于本章目录的 `data/` 下，以压缩包形式提供（data.tar.gz）。由于 TUM 数据集是从实际环境中采集的，需要解释一下它的数据格式（数据集一般都有自己定义的格式）。在解压后，你将看到以下这些文件：
1. `rgb.txt` 和 `depth.txt` 记录了各文件的采集时间和对应的文件名。
2. `rgb/` 和 `depth/` 目录存放着采集到的 png 格式图像文件。彩色图像为 `8` 位三通道，深
度图为 `16` 位单通道图像。文件名即采集时间。
3. `groundtruth.txt` 为外部运动捕捉系统采集到的相机位姿，格式为

$$(time, t_x, t_y, t_z, q_x, q_y, q_z, q_w)$$

我们可以把它看成标准轨迹。

请注意 `彩色图`、`深度图` 和 `标准轨迹` 的采集都是独立的，轨迹的采集频率比图像高很多。在使用数据之前，需要根据采集时间，对数据进行一次时间上的对齐，以便对彩色图和深度图进行配对。原则上，我们可以把采集时间相近于一个阈值的数据，看成是一对图像。并把相近时间的位姿，看作是该图像的真实采集位置。TUM 提供了一个 python 脚本 `associate.py`（或使用 slambook/tools/associate.py）帮我们完成这件事。

>associate.py
- 原代码有一个小bug
- 运行
    ```shell
    python [associate.py的路径] [rgb.txt路径] [depth.txt] > [associate.txt路径]
    ```
- 这段脚本会根据输入两个文件中的采集时间进行配对，最后输出到一个文件 associate.txt。输出文件含有被配对的两个图像的时间、文件名信息，可以作为后续处理的来源。此外，TUM 数据集还提供了比较估计轨迹与标准轨迹的工具，我们将在用到的地方再进行介绍。
```py
import argparse
import sys
import os
import numpy

def read_file_list(filename):
    """
    Reads a trajectory from a text file. 
    
    File format:
    The file format is "stamp d1 d2 d3 ...", where stamp denotes the time stamp (to be matched)
    and "d1 d2 d3.." is arbitary data (e.g., a 3D position and 3D orientation) associated to this timestamp. 
    
    Input:
    filename -- File name
    
    Output:
    dict -- dictionary of (stamp,data) tuples
    
    """
    file = open(filename)
    data = file.read()
    lines = data.replace(","," ").replace("\t"," ").split("\n") 
    list = [[v.strip() for v in line.split(" ") if v.strip()!=""] for line in lines if len(line)>0 and line[0]!="#"]
    list = [(float(l[0]),l[1:]) for l in list if len(l)>1]
    return dict(list)

def associate(first_list, second_list,offset,max_difference):
    """
    Associate two dictionaries of (stamp,data). As the time stamps never match exactly, we aim 
    to find the closest match for every input tuple.
    
    Input:
    first_list -- first dictionary of (stamp,data) tuples
    second_list -- second dictionary of (stamp,data) tuples
    offset -- time offset between both dictionaries (e.g., to model the delay between the sensors)
    max_difference -- search radius for candidate generation

    Output:
    matches -- list of matched tuples ((stamp1,data1),(stamp2,data2))
    
    """
    first_keys = list(first_list.keys())
    second_keys = list(second_list.keys())
    potential_matches = [(abs(a - (b + offset)), a, b) 
                         for a in first_keys 
                         for b in second_keys 
                         if abs(a - (b + offset)) < max_difference]
    potential_matches.sort()
    matches = []
    for diff, a, b in potential_matches:
        if a in first_keys and b in second_keys:
            first_keys.remove(a)
            second_keys.remove(b)
            matches.append((a, b))
    
    matches.sort()
    return matches

if __name__ == '__main__':

    # parse command line
    parser = argparse.ArgumentParser(description='''
    # This script takes two data files with timestamps and associates them   
    # ''')
    parser.add_argument('first_file', help='first text file (format: timestamp data)')
    parser.add_argument('second_file', help='second text file (format: timestamp data)')
    parser.add_argument('--first_only', help='only output associated lines from first file', action='store_true')
    parser.add_argument('--offset', help='time offset added to the timestamps of the second file (default: 0.0)',default=0.0)
    parser.add_argument('--max_difference', help='maximally allowed time difference for matching entries (default: 0.02)',default=0.02)
    args = parser.parse_args()
    first_list = read_file_list(args.first_file)
    second_list = read_file_list(args.second_file)


    matches = associate(first_list, second_list,float(args.offset),float(args.max_difference))    

    if args.first_only:
        for a,b in matches:
            print("%f %s"%(a," ".join(first_list[a])))
    else:
        for a,b in matches:
            print("%f %s %f %s"%(a," ".join(first_list[a]),b-float(args.offset)," ".join(second_list[b])))
```

&emsp;
## 8.3.2 使用 LK 光流
下面我们来编写程序使用 $OpenCV$ 中的 $LK$ 光流。使用 $LK$ 的目的是跟踪特征点。我们对第一张图像提取 $FAST$ 角点，然后用 $LK$ 光流跟踪它们，并画在图中。

>头文件
```c++
#include <iostream>
#include <fstream>
#include <list>
#include <vector>
#include <chrono>
using namespace std; 

#include <opencv2/core/core.hpp>
#include <opencv2/highgui/highgui.hpp>
#include <opencv2/features2d/features2d.hpp>
#include <opencv2/video/tracking.hpp>
```

>库
```c++
opencv_video
```

>main.cpp
```c++
#include "8.3_useLK.hpp"

int main( int argc, char** argv )
{
    string path_to_dataset = "./data";
    string associate_file = "./associate.txt";
    
    ifstream fin( associate_file );
    if ( !fin ) 
    {
        cerr<<"I cann't find associate.txt!"<<endl;
        return 1;
    }
    
    string rgb_file, depth_file, time_rgb, time_depth;
    std::list< cv::Point2f > keypoints;      // 因为要删除跟踪失败的点，使用list
    cv::Mat color, depth, last_color;
    
    for ( int index=0; index<100; index++ )
    {
        fin>>time_rgb>>rgb_file>>time_depth>>depth_file;
        color = cv::imread( path_to_dataset+"/"+rgb_file );
        depth = cv::imread( path_to_dataset+"/"+depth_file, -1 );
        if (index ==0 )
        {
            // 对第一帧提取FAST特征点
            vector<cv::KeyPoint> kps;
            cv::Ptr<cv::FastFeatureDetector> detector = cv::FastFeatureDetector::create();
            detector->detect( color, kps );
            for ( auto kp:kps )
                keypoints.push_back( kp.pt );
            last_color = color;
            continue;
        }
        if ( color.data==nullptr || depth.data==nullptr )
            continue;
        // 对其他帧用LK跟踪特征点
        vector<cv::Point2f> next_keypoints; 
        vector<cv::Point2f> prev_keypoints;
        for ( auto kp:keypoints )
            prev_keypoints.push_back(kp);
        vector<unsigned char> status;
        vector<float> error; 
        chrono::steady_clock::time_point t1 = chrono::steady_clock::now();
        cv::calcOpticalFlowPyrLK( last_color, color, prev_keypoints, next_keypoints, status, error );
        chrono::steady_clock::time_point t2 = chrono::steady_clock::now();
        chrono::duration<double> time_used = chrono::duration_cast<chrono::duration<double>>( t2-t1 );
        cout<<"LK Flow use time："<<time_used.count()<<" seconds."<<endl;
        // 把跟丢的点删掉
        int i=0; 
        for ( auto iter=keypoints.begin(); iter!=keypoints.end(); i++)
        {
            if ( status[i] == 0 )
            {
                iter = keypoints.erase(iter);
                continue;
            }
            *iter = next_keypoints[i];
            iter++;
        }
        cout<<"tracked keypoints: "<<keypoints.size()<<endl;
        if (keypoints.size() == 0)
        {
            cout<<"all keypoints are lost."<<endl;
            break; 
        }
        // 画出 keypoints
        cv::Mat img_show = color.clone();
        for ( auto kp:keypoints )
            cv::circle(img_show, kp, 10, cv::Scalar(0, 240, 0), 1);
        cv::imshow("corners", img_show);
        cv::waitKey(0);
        last_color = color;
    }
    return 0;
}
```


我们会在每次循环后暂停程序，按任意键可以继续运行。你会看到图像中大部分特征点能够顺利跟踪到，但也有特征点会丢失。丢失的特征点或是被移出了视野外，或是被其他物体挡住了。如果我们不提取新的特征点，那么光流的跟踪会越来越少：

<div align="center">
    <image src="./imgs/8.3-1.png" width = 600>
</div>
&emsp;

图 8-2 显示了程序运行过程中若干帧的情况（这里使用了完整的数据集，但本书的 git 上只给出了十张图）。

最初我们大约有 $1700$ 个特征点。跟踪过程中一部分特征点会丢失，直到 100 帧时我们还有约 $178$ 个特征点，相机视角相对于最初的图像也发生了较大改变。

仔细观察特征点的跟踪过程，我们会发现位于物体角点处的特征更加稳定。边缘处的特征会沿着边缘“滑动”，这主要是因为沿着边缘移动时特征块的内容基本不变，因此程序容易认为是同一个地方。而既不在角点，也不在边缘的的特征点则会频繁跳动，位置非常不稳定。这个现象很像围棋中的“金角银边草肚皮”：角点具有更好的辨识度，边缘次之，区块最少。

另一方面，读者可以看到光流法的运行时间。在跟踪 $1500$ 个特征点时，$LK$ 光流法大约需要 $20$ 毫秒左右。如果减小特征点的数量，则会明显减少计算时间。我们看到，$LK$ 光流跟踪法避免了描述子的计算与匹配，但本身也需要一定的计算量。在我们的计算平台上，使用 $LK$ 光流能够节省一定的计算量，但在具体 SLAM 系统中使用光流还是匹配描述子，
最好是亲自做实验测试一下。

另外，$LK$ 光流跟踪能够直接得到特征点的对应关系。这个对应关系就像是描述子的匹配，但实际上我们大多数时候只会碰到特征点跟丢的情况，而不太会遇到误匹配，这应