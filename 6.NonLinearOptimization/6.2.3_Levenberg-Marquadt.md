&emsp;
# 2.3 Levenberg-Marquadt

由于 Gauss-Newton 方法中采用的近似二阶泰勒展开只能在展开点附近有较好的近似效果，所以我们很自然地想到应该给 $∆\pmb{x}$ 添加一个信赖区域（Trust Region），不能让它太大而使得近似不准确。非线性优化种有一系列这类方法，这类方法也被称之为 `信赖区域方法 (Trust Region Method)`。

在信赖区域里边，我们认为近似是有效的；出了这个区域，近似可能会出问题。那么如何确定这个信赖区域的范围呢？一个比较好的方法是根据我们的近似模型跟实际函数之间的差异来确定这个范围：如果差异小，我们就让范围尽可能大；如果差异大，我们就缩小这个近似范围。因此，考虑使用

$$\rho = \frac{f(\pmb{x} + ∆\pmb{x})  - f(\pmb{x})}
{\pmb{J}(\pmb{x})∆\pmb{x}}$$


来判断泰勒近似是否够好。$ρ$ 的分子是实际函数下降的值，分母是近似模型下降的值。如果 $ρ$ 接近于 $1$，则近似是好的。如果 $ρ$ 太小，说明实际减小的值远少于近似减小的值，则认为近似比较差，需要缩小近似范围。反之，如果 $ρ$ 比较大，则说明实际下降的比预计的更大，我们可以放大近似范围。

于是，我们构建一个改良版的非线性优化框架，该框架会比 Gauss Newton 有更好的
效果：

1. 给定初始值 $\pmb{x}_0$，以及初始优化半径 $µ$
2. 对于第 $k$ 次迭代，求解：
    $$\min_{∆\pmb{x}_k} \frac{1}{2} ∥f(\pmb{x}_k) + \pmb{J} (\pmb{x}_k) ∆\pmb{x}_k∥^2
    ， s.t.||\pmb{D}∆\pmb{x}_k||^2 ≤ µ, (6.24)$$
    这里 $µ$ 是信赖区域的半径，$\pmb{D}$ 将在后文说明

3. 计算 $ρ$
4. 若 $ρ > \frac{3}{4}$，则 $µ = 2µ$
5. 若 $ρ < \frac{1}{4}$，则 $µ = 0.5µ$
6. 如果 $ρ$ 大于某阈值，认为近似可行。令 $\pmb{x}_{k+1} = \pmb{x}_k + ∆\pmb{x}_k$
7. 判断算法是否收敛。如不收敛则返回 $2$，否则结束

这里近似范围扩大的倍数和阈值都是经验值，可以替换成别的数值。在式$（6.24）$中，我们把增量限定于一个半径为 $\mu$ 的球中，认为只在这个球内才是有效的。带上 $\pmb{D}$ 之后，这个球可以看成一个椭球。

在 Levenberg 提出的优化方法中，把 $\pmb{D}$ 取成单位阵 $\pmb{I}$，相当于直接把 $∆\pmb{x}$ 约束在一个球中。随后，Marqaurdt 提出将 $\pmb{D}$ 取成非负数对角阵——实际中通常用 $\pmb{J}^T\pmb{J}$ 的对角元素平方根，使得在梯度小的维度上约束范围更大一些。

不论如何，在 L-M 优化中，我们都需要解式$（6.24）$那样一个子问题来获得梯度。这个子问题是带不等式约束的优化问题，我们用 Lagrange 乘子将它转化为一个无约束优化问题：


$$\min_{∆\pmb{x}_k} \frac{1}{2} ∥f(\pmb{x}_k) + \pmb{J} (\pmb{x}_k) ∆\pmb{x}_k∥^2 +
\frac{\lambda}{2}||\pmb{D}∆\pmb{x}||^2$$

这里 $λ$ 为 Lagrange 乘子。类似于 Gauss-Newton 中的做法，把它展开后，我们发现该问题的核心仍是计算增量的线性方程：

$$(\pmb{H} + \lambda\pmb{D}^T\pmb{D})∆\pmb{x} = \pmb{g}$$

可以看到，增量方程相比于 Gauss-Newton，多了一项 $\ λ\pmb{D}^T\pmb{D}$。如果考虑它的简化形式，即 $\pmb{D} = \pmb{I}$，那么相当于求解：


$$(\pmb{H} + \lambda\pmb{I})∆\pmb{x} = \pmb{g}$$

我们看到，当参数 $λ$ 比较小时，$\pmb{H}$ 占主要地位，这说明二次近似模型在该范围内是比较好的，L-M 方法更接近于 G-N 法。另一方面，当 $λ$ 比较大时，$λ\pmb{I}$ 占据主要地位，L-M更接近于一阶梯度下降法（即最速下降），这说明附近的二次近似不够好。

L-M 的求解方式，可在一定程度上避免线性方程组的系数矩阵的非奇异和病态问题，提供更稳定更准确的增量 $∆\pmb{x}$。在

