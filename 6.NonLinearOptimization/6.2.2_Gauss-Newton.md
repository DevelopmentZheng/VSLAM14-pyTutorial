&emsp;
# 2.2 Gauss-Newton

`Gauss Newton` 是最优化算法里面最简单的方法之一。它的思想是将 $f(\pmb{x})$ 进行一阶的泰勒展开（请注意不是 `一阶梯度法` 采用目标函数 $f(\pmb{x})^2$：

$$f(\pmb{x} + \triangle \pmb{x}) \approx f(\pmb{x}) + \pmb{J}(\pmb{x})\triangle \pmb{x}$$

这里 $\pmb{J}(\pmb{x})$ 为 $f(\pmb{x})$ 关于 $\pmb{x}$ 的导数，实际上是一个 $m × n$ 的矩阵，也是一个雅可比矩阵。根据前面的框架，当前的目标是为了寻找下降矢量 $∆\pmb{x}$，使得 $||f (\pmb{x} + ∆\pmb{x})||^2$ 达到最小。为了求 $∆\pmb{x}$，我们需要解一个线性的最小二乘问题：

$$\triangle \pmb{x}* = arg \min_{\triangle \pmb{x}}
\frac{1}{2}||f(\pmb{x}) + \pmb{J}(\pmb{x}) \triangle \pmb{x}||^2_2 $$

这个方程与之前有什么不一样呢？根据极值条件，将上述目标函数对 $∆\pmb{x}$ 求导，并令导数为零。由于这里考虑的是 $∆\pmb{x}$ 的导数（而不是 $\pmb{x}$），我们最后将得到一个线性的方程。

为此，先展开目标函数的平方项：

$$\frac{1}{2}||f(\pmb{x}) + \pmb{J}(\pmb{x}) \triangle \pmb{x}||^2_2 = 
\frac{1}{2}\Big( f(\pmb{x}) + \pmb{J}(\pmb{x}) \triangle \pmb{x}\Big)^T \Big(f(\pmb{x}) + \pmb{J}(x)\triangle \pmb{x}\Big)$$

$$\qquad\qquad\qquad\qquad\qquad\qquad\ = \frac{1}{2} \Bigg( 
    ||f(\pmb{x})||^2_2 +
     2f(\pmb{x})^T\pmb{J}(\pmb{x}) \triangle \pmb{x} 
     + \triangle \pmb{x}^T\pmb{J}(\pmb{x})^T \pmb{J}(\pmb{x}) \triangle \pmb{x}
\Bigg)$$

求上式关于 $∆\pmb{x}$ 的导数，并令其为零：

$$2\pmb{J}(\pmb{x})^T f(\pmb{x}) +
2\pmb{J}(\pmb{x})^T\pmb{J}(\pmb{x}) \triangle \pmb{x} = \pmb{0}$$


可以得到如下方程组：

$$\pmb{J}(\pmb{x})^T\pmb{J}(\pmb{x}) \triangle \pmb{x} = 
- \pmb{J}(\pmb{x})^T f(\pmb{x})$$

注意，我们要求解的变量是 $∆\pmb{x}$，因此这是一个线性方程组，我们称它为 `增量方程`，也可以称为 `高斯牛顿方程 (Gauss Newton equations)` 或者 `正规方程 (Normal equations)`。我们把左边的系数定义为 $\pmb{H}$，右边定义为 $\pmb{g}$，那么上式变为：

$$\pmb{H}\triangle \pmb{x} = \pmb{g}$$

这里把左侧记作 $\pmb{H}$ 是有意义的。对比牛顿法可见，Gauss-Newton 用 $\pmb{J}^T \pmb{J}$ 作为牛顿法中二阶 Hessian 矩阵的近似，从而省略了计算 $\pmb{H}$ 的过程。求解增量方程是整个优化问题的核心所在。如果我们能够顺利解出该方程，那么 Gauss-Newton 的算法步骤可以写成：


1. 给定初始值 $\pmb{x}_0$
2. 对于第 $k$ 次迭代，求出当前的雅可比矩阵 $\pmb{J}_(\pmb{x}_k)$ 和误差 $f(\pmb{x}_k)$
3. 求解增量方程：$\pmb{H}∆\pmb{x}_k = \pmb{g}$
4. 若 $∆\pmb{x}_k$ 足够小，则停止。否则，令 $\pmb{x}_{k+1} = \pmb{x}_k + ∆\pmb{x}_k$，返回 $2$

从算法步骤中可以看到，增量方程的求解占据着主要地位。原则上，它要求我们所用
的近似 $\pmb{H}$ 矩阵是可逆的（而且是正定的），但实际数据中计算得到的 $\pmb{J}^T \pmb{J}$ 却只有半正定性。

也就是说，在使用 Gauss Newton 方法时，可能出现 $\pmb{J}^T \pmb{J}$ 为奇异矩阵或者病态 (ill-condition) 的情况，此时增量的稳定性较差，导致算法不收敛。更严重的是，就算我们假设 $\pmb{H}$ 非奇异也非病态，如果我们求出来的步长 $∆\pmb{x}$ 太大，也会导致我们采用的局部近似(6.19) 不够准确，这样一来我们甚至都无法保证它的迭代收敛，哪怕是让目标函数变得更大都是有可能的。


尽管 Gauss Newton 法有这些缺点，但是它依然值得我们去学习，因为在非线性优化里，相当多的算法都可以归结为 Gauss Newton 法的变种。这些算法都借助了 Gauss Newton 法的思想并且通过自己的改进修正 Gauss Newton 法的缺点。例如一些线搜索方法 (line search method)，这类改进就是加入了一个标量 α，在确定了 $\pmb{∆x}$ 进一步找到 α使得 $||f(\pmb{x} + α\pmb{∆x})||^2$ 达到最小，而不是像 Gauss Newton 法那样简单地令 α = 1。

&emsp;
>Levenberg-Marquadt 的引出
- Levenberg-Marquadt 方法在一定程度上修正了这些问题

    一般认为它比 Gauss Newton 更为鲁棒。尽管它的收敛速度可能会比 Gauss Newton 更慢，被称之为 `阻尼牛顿法(Damped Newton Method)`，但是在 SLAM 里面却被大量应用。



