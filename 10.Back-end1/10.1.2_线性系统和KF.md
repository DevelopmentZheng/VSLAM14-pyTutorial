&emsp;
# 线性系统和KF
我们首先来看滤波器模型。当我们假设了马尔可夫性，从数学角度会发生哪些变化呢？

首先，当前时刻状态只和上一个时刻有关，式（10.6）中等式右侧第一部分可进一步简化：

$$P(\pmb{x}_k | \pmb{x}_{k-1}，\pmb{x}_0，\pmb{u}_{1:k}，\pmb{z}_{1:k-1}) = P(\pmb{x}_k | \pmb{x}_{k-1}，\pmb{u}_{1:k})$$

这里，由于 $k$ 时刻状态与 $k - 1$ 之前的无关，所以就简化成只与 $\pmb{x}_{k-1}$ 和 $\pmb{u}_k$ 有关的形式，与 $k$ 时刻的运动方程对应。第二部分可简化为：

$$P(\pmb{x}_k | \pmb{x}_{k-1}，\pmb{x}_0，\pmb{u}_{1:k}，\pmb{z}_{1:k-1}) = 
P(\pmb{x}_{k-1} | \pmb{x}_0，\pmb{u}_{1:k-1}，\pmb{z}_{1:k-1})$$

这是考虑到 $k$ 时刻的输入量 $\pmb{u}_k$ 与 $k - 1$ 时刻的状态无关，所以我们把 $\pmb{u}_k$ 拿掉。可以看到，这一项实际是 $k - 1$ 时刻的状态分布

于是，这一系列方程说明了，我们实际在做的是"如何把 $k - 1$ 时刻的状态分布推导至 $k$ 时刻"这样一件事。也就是说，在程序运行期间，我们只要维护一个状态量，对它进行不断地迭代和更新即可。进一步，如果假设状态量服从高斯分布，那我们只需考虑维护状态量的均值和协方差即可。

&emsp;
## 线性高斯系统
我们从形式最简单的线性高斯系统开始，最后会得到卡尔曼滤波器。线性高斯系统是说，运动方程和观测方程可以由线性方程来描述：

$$\begin{cases} \pmb{x}_k = \pmb{A}_k\pmb{x}_{k-1} + \pmb{u}_k + \pmb{w}_k \\
\pmb{z}_k = \pmb{C}_k\pmb{x}_k + \pmb{v}_k\end{cases}
\qquad k = 1，...，N$$

并假设所有的状态和噪声均满足高斯分布。记这里的噪声服从零均值高斯分布：

$$\pmb{w}_k \backsim N(\pmb{0}, \pmb{R})，\pmb{v}_k \backsim N(\pmb{0}, \pmb{Q})$$

为了简洁我省略了 $\pmb{R}$ 和 $\pmb{Q}$ 的下标。现在，利用马尔可夫性，假设我们知道了 $k - 1$ 时刻的后验（在 $k - 1$ 时刻看来）状态估计：$\hat{\pmb{x}}_{k- 1}$ 和它的协方差 $\hat{\pmb{P}}_{k-1}$，现在要根据 $k$ 时刻的输入和观测数据，确定 $\hat{\pmb{x}}_k$ 的后验分布。为区分推导中的先验和后验，我们在记号上作一点区别：
- $\hat{\pmb{x}}_k$ 表示后验
- $\bar{x}$ 表示先验分布

&emsp;
## 卡尔曼滤波推导
>运动方程
- 卡尔曼滤波器的第一步，通过运动方程确定 $\pmb{x}_k$ 的先验分布。根据高斯分布的性质，显然有：

    $$P(\pmb{x}_k | \pmb{x}_{k-1}，\pmb{x}_0，\pmb{u}_{1:k}，\pmb{z}_{1:k-1}) = N(\pmb{A}_k\hat{\pmb{x}}_{k-1} + \pmb{u}_k，\pmb{A}_k\hat{\pmb{P}}_{k-1}\pmb{A}^T_k + \pmb{R})$$

    这一步称为预测。它显示了如何从上一个时刻的状态，根据输入信息（但是有噪声），推断当前时刻的状态分布。这个分布也就是先验。记这里的：

    $$\bar{\pmb{x}}_k = \pmb{A}\hat{x}_{k-1} + \pmb{u}_k，\bar{\pmb{P}}_k = \pmb{A}_k\hat{\pmb{P}}_{k-1}\pmb{A}^T + \pmb{R}$$

>观测方程
- 另一方面，由观测方程，我们可以计算在某个状态下，应该产生怎样的观测数据：

    $$P(z_k | x_k) = N(\pmb{C}_k\pmb{x}_k，\pmb{Q})$$

    为了得到后验概率，我们想要计算它们的乘积，也就是由式

    $$P(\pmb{x}_k | \pmb{x}_0，\pmb{u}_{1:k}，\pmb{z}_{1:k}) ∝ P(\pmb{z}_k | \pmb{x}_k) P(\pmb{x}_k | \pmb{x}_0，\pmb{u}_{1:k}，\pmb{z}_{1:k-1})$$

    给出的贝叶斯公式。然而，虽然我们知道最后会得到一个关于 $x_k$ 的高斯分布，但计算上是有一丁点儿麻烦的，我们先把结果设为 $x_k ∼ N(\hat{x}_k, \hat{P}_k)$，那么：

    $$N(\hat{x}_k, \hat{P}_k) = N(C_kx_x，Q)\cdot N(\bar{x}_k，\bar{P}_k)$$

这里我们稍微用点讨巧的方法。既然我们已经知道等式两侧都是高斯分布，那就只需比较指数部分即可，而无须理会高斯分布前面的因子部分。指数部分很像是一个二次型的配方，我们来推导一下。首先把指数部分展开，有

$$(\pmb{x}_k - \hat{\pmb{x}}_k)^T \hat{\pmb{P}^{-1}}_k(\pmb{x}_k - \hat{\pmb{x}}_k) = 
(z_k - C_kx_k)^TQ^{-1}(z_k - C_kx_k) + 
(x_k - C_k\bar{x}_k)^T\bar{P}^{-1}_k(x_k - C_k\bar{x}_k)$$

为了求左侧的 $\hat{\pmb{x}}_k$ 和 $\hat{\pmb{P}}_k$，我们把两边展开，并比较 $\pmb{x}_k$ 的二次和一次系数。对于二次系数，有：

$$\hat{\pmb{P}}^{-1}_k = \pmb{C}^T_k\pmb{Q}^{-1}\pmb{C}_k + \bar{P}^{-1}_k \qquad (10.16)$$

该式给出了协方差的计算过程。为了便于后边列写式子，定义一个中间变量：

$$\pmb{K} = \hat{\pmb{P}}_k\pmb{C}^T_k\pmb{Q}^{-1}$$

根据此定义，在式（10.16）左右各乘 $\hat{\pmb{P}}_k$，有：
$$\pmb{I} = \hat{\pmb{P}}_k\pmb{C}^T_k\pmb{Q}^{-1}\pmb{C}_k + \hat{\pmb{P}}_k\bar{\pmb{P}}^{-1}_k  \\
= \pmb{KC}_k + \hat{\pmb{P}}_k\bar{\pmb{P}}^{-1}_k $$

于是有：
$$\hat{\pmb{P}}_k = (\pmb{I}-\pmb{KC}_k)\bar{\pmb{P}}$$

然后再比较一次项的系数，有：
$$-2\hat{x}_k^T\hat{\pmb{P}}_k^{-1}x_k = -2z_k^T\pmb{Q}^{-1}C_kx_k - 2\bar{x}^T_k\bar{P}_k^{-1}x_k$$

整理（取系数并转置）得：
$$\hat{P}_k^{-1}\hat{x}_k = C^T_kQ^{-1}z_k + \bar{P}_k^{-1}\bar{x}_k$$

两侧乘以 $\hat{\pmb{P}}$ 并代入式（10.17）得：
$$\hat{\pmb{x}}_k = \hat{\pmb{P}}_k\pmb{C}^T_k\pmb{Q}^{-1}z_k + \hat{P}_k\bar{P}_k\bar{x}_k \\
= \pmb{K}z_k + (\pmb{I}-\pmb{KC}_k)\bar{\pmb{x}}_k\\
= \bar{x}_k + \pmb{K}(z_k - C_k\bar{x}_k)$$

于是我们又得到了后验均值的表达

&emsp;
## 总结
上面的两个步骤可以归纳为以下两个步骤：
- 预测（Predict）
- 更新（Update）


>预测
$$\bar{\pmb{x}}_k = \pmb{A}\hat{x}_{k-1} + \pmb{u}_k，\bar{\pmb{P}}_k = \pmb{A}_k\hat{\pmb{P}}_{k-1}\pmb{A}^T + \pmb{R}$$

>更新
- 先计算 $\pmb{K}$，卡尔曼增益
$$\pmb{K} = \bar{\pmb{P}}_k\pmb{C}^T_k(\pmb{C}_k\bar{P}_k\pmb{C}^T_k+\pmb{Q})^{-1}$$
- 然后计算后验概率分布
$$\hat{\pmb{x}}_k = \bar{x}_k + \pmb{K}(z_k - \pmb{C}_k\bar{x}_k)$$
$$\hat{\pmb{P}}_k = (\pmb{I} - \pmb{KC}_k)\bar{P}_k$$

卡尔曼滤波器有若干种推导方式，我们使用的是从概率角度出发的最大后验概率估计的形式。

我们看到，在线性高斯系统中，卡尔曼滤波器构成了该系统中的最大后验概率估计。而且，由于高斯分布经过线性变换后仍服从高斯分布，所以整个过程中我们没有进行任何的近似。可以说，卡尔曼滤波器构成了线性系统的最优无偏估计。